{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Introduction to LSTM Networks\n",
    "#### Peter Schneider (peter.schneider@soma-analytics.com) & David Haber (david@cognitir.com) - Deep Learning Meetup - 24 June 2016 / Berlin Machine Learning Meetup, 1 August 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "### Limitations of Feed-Forward Neural Networks\n",
    "\n",
    "- Feed-forward neural networks can't make use of time information, but when modeling time series such as audio signal, stock markets or sensor signals, time information is crucial to successful modeling.\n",
    "- Sequences are an integral part of intelligence. Human intelligence is based on sequential pattern recognition. Can you say the alphabet backwards?\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/2000px-Colored_neural_network.svg.png\", width=200>\n",
    "\n",
    "- Recurrent Neural Networks offer a way to model time information by allowing cyclical connections.\n",
    "\n",
    "### Prominent Applications of Recurrent Neural Networks\n",
    "\n",
    "- Speech Processing/Recognition\n",
    "    - Google applies RNNs in their email auto-response technology\n",
    "    - Apple uses RNNs for speech recognition in Siri\n",
    "- Finance\n",
    "    - Recurrent Nets have been successfully applied in automatic trading systems\n",
    "- Music Composition\n",
    "    - http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/\n",
    "- Motor Control\n",
    "- Biological Sequence Analysis\n",
    "- Machine Translation\n",
    "- Reinforcement Learning\n",
    "- Meta Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation - A method to compute the gradient\n",
    "\n",
    "* Gradient descent can help us to optimize weights in a neural network.\n",
    "* To perform learning we must compute the gradient first before we can upate weights and biases\n",
    "$\\rightarrow$ this is what backpropagation does.\n",
    "* It applies the chain rule to compute the updates layer by layer from the top to the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume following cost function for a single training example x:\n",
    "\n",
    "$$ C = \\frac{1}{2} \\sum_{j} (y_j - a_j^L)^2 $$\n",
    "\n",
    "with\n",
    "\n",
    "$$ a^l = \\sigma(z^l) $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ z^l \\equiv w^l a^{l-1} + b^l$$\n",
    "\n",
    "and define\n",
    "\n",
    "$$ \\delta_j^l \\equiv \\frac{\\partial C}{\\partial z_j^l}  $$\n",
    "\n",
    "#### Error at Output Layer: $\\delta^L_j$\n",
    "\n",
    "Goal: Compute gradient of output layer\n",
    "\n",
    "\\begin{align}\n",
    "\\delta^L_j & = \\frac{\\partial C}{\\partial z^L_{j}} \\\\\n",
    "& = y_j - \\sigma'(z_j^L)\n",
    "\\end{align}\n",
    "\n",
    "#### Recursive Computation of Error at Hidden Layers: $\\delta_j^l$\n",
    "\n",
    "Goal: Make $\\delta^l$ a function of $\\delta^{l+1}$ to recursively compute gradients in lower layers\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{align}\n",
    "z^{l+1}_k & = \\sum_{m} w_{km}^{l+1} a^l_k + b_j^{l+1} \\\\\n",
    "& = \\sum_{m} w_{km}^{l+1} \\sigma(z_m^l) + b_j^{l+1}\n",
    "\\end{align}\n",
    "\n",
    "So to compute the full gradient of $z_j^l$ we need to gather the gradients of all higher layers\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial C}{\\partial z^l_{j}} & = \\sum_k \\frac{\\partial C}{\\partial z^{l+1}_{k}} \\frac{\\partial z^{l+1}_{k}}{\\partial z^l_{j}} \\\\\n",
    "& = \\sum_k \\delta_k^{l+1} w_{jk}^{l+1} \\sigma'(z_j^l)\n",
    "\\end{align}\n",
    "\n",
    "the result is: $\\delta_j^l = \\sum_k \\delta_k^{l+1} w_{jk}^{l+1} \\sigma'(z_j^l)$\n",
    "\n",
    "\n",
    "#### Weight Update\n",
    "\n",
    "\\begin{align} \n",
    "\\frac{\\partial C}{\\partial w^l_{jk}} & = \\frac{\\partial C}{\\partial z^l_{j}} \\frac{\\partial z_j^l}{\\partial w^l_{jk}} \\\\\n",
    "&= \\delta_j^l a_k^{l-1}\n",
    "\\end{align}\n",
    "\n",
    "note that $z^l_j = \\sum_{k}w_{jk}a_k^{l-1} + b_j$\n",
    "\n",
    "#### Bias Update\n",
    "\n",
    "\\begin{align} \n",
    "\\frac{\\partial C}{\\partial b^l_{j}} & = \\frac{\\partial C}{\\partial z^l_{j}} \\frac{\\partial z_j^l}{\\partial b^l_{j}} \\\\\n",
    "&= \\delta_j^l\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem - A heuristic approach\n",
    "\n",
    "Suppose we have a network with 4 layers, a single unit per layer and use the sigmoid activation function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "![alternate text](single_layer_single_unit_nn.png)\n",
    "\n",
    "Furthermore, suppose we want to compute the gradient for the bias of the first layer $\\frac{\\partial C}{\\partial b^l_{j}}$.\n",
    "\n",
    "First it is important to note that $\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$ is always $\\leq 0.25$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show, gridplot, output_notebook\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1. / (1. + np.exp(-x)) \n",
    "\n",
    "p = figure(title=\"Activation Functions\", plot_width=600, plot_height=400)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y_s = np.multiply(sigmoid(x),(1.-sigmoid(x)))\n",
    "y_t = 1. - np.tanh(x)**2\n",
    "p.line(x, y_s, color='blue', legend='first derivative of sigmoid')\n",
    "p.line(x, y_t, color='green', legend='first derivative of tanh')\n",
    "output_notebook()\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recap that $\\frac{\\partial C}{\\partial b^l_{j}} = \\delta_j^l$ so it follows from recursive substitution in $\\delta^l = \\frac{\\partial C}{\\partial a^L} \\sigma'(z^L) \\prod^{L-1}_{i=l} w^{i+1} \\sigma'(z^i)$\n",
    "* If weights are initialized to $w \\sim \\mathcal{N}(0,1)$ then most likely $|w| < 4$ and $\\left|w \\sigma'(z)\\right| < 1 $\n",
    "* It follows $\\prod_{i=j}^{L-1} w^i \\sigma'(z^i)$ will converge to zero for the lower layers in a deep neural network $\\rightarrow$ so the gradients in the lower layer tend to **vanish** (keep in mind: $\\sigma'(x) \\leq 0.25$). \n",
    "\n",
    "### Exploding gradient problem\n",
    "\n",
    "* If $\\left\\vert w \\sigma'(z) \\right\\vert > 1$ then the product will explode.\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. In general it is hard to control the vanishing and the exploding gradient problem as we would have to maintain a range of values for the input and the weights to control the gradient.\n",
    "\n",
    "2. The problem seems to be severe especially with the sigmoid activation function (Glorot, Bengio: \"Understanding the difficulty of training deep feedforward neural networks\", 2010)\n",
    "\n",
    "3. One could argue that using a tanh activation might be a better option, but keep in mind that the derivative of tanh tends to go faster to zero than the sigmoid (x > $\\left\\vert 1.6 \\right\\vert$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "* Feedforward Neural Networks don't possess an internal state, so once an input is processed the network \"forgets\" about it.\n",
    "\n",
    "* In time series processing, this is a drawback as information that is in the time structure is lost.\n",
    "\n",
    "* Recurrent Neural Networks (RNNs) try to bridge the gap by allowing cyclical connections $$ z^{l,t}_i = w'^l_{i} a^{l-1, t} + u'^l_{i} a^{l, t-1} + b^l_i $$ and $$ a^{l,t}_i = \\sigma(z^{l,t}_i) $$.\n",
    "\n",
    "* through $ u'^l_{i} a^{l, t-1}$ (non) linear time dependencies are captured.\n",
    "\n",
    "\n",
    "\n",
    "#### Backpropagation Through Time\n",
    "\n",
    "* Conventional BPTT (Williams and Zipser, 1992) is similar to normal backpropagation but unrolls the neural network over time:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN01.png\", width=550>\n",
    "\n",
    "* The time-dependent derivative is as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "\\delta^{t,l}_h = \\sigma'(z^{l,t}_h) \\left( \\sum_{k=1}^{K} \\delta^{t,l+1}_k w_{hk}^{l+1} + \\sum_{h'=1}^{H} \\delta_{h'}^{t+1,l} w_{hh'} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "* As the weights are the same for every time step, we have to sum over them in order to get weight and bias derivative: \n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial w^{l}_{i,j}} = \\sum_{t=1}^{T} \\frac{\\partial C}{\\partial z^{t,l}_j} \\frac{\\partial z^{t,l}_j}{\\partial w^{l}_{i,j}} = \\sum_{t=1}^{T} \\delta_j^{t,l} \\sigma(a^{l,t}_j)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial C}{\\partial b^{l}_{j}} =\\sum_{t=1}^{T} \\frac{\\partial C}{\\partial z^{t,l}_j} \\frac{\\partial z^{t,l}_j}{\\partial b^{l}_{j}} = \\sum_{t=1}^{T} \\delta_j^{t,l}\n",
    "\\end{equation}\n",
    "\n",
    "* Backpropagation through time exhibits the *same issues* with a vanishing or exploding gradient (Hochreiter, 1991) even for a small number of time steps (short-term memory) $\\rightarrow$, therefore it is hard to capture long-range time dependencies (long-term memory).\n",
    "\n",
    "* To circumvent this problem, Hochreiter and Schmidhuber (1997) modified recurrent units into long-short term memory units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Long Short Term Memory Networks (LSTMs)\n",
    "\n",
    "- Learning to store information over extended time intervals (long-range time dependencies) via recurrent backpropagation is difficult.\n",
    "- LSTMs enforces *constant* (neither exploding nor vanishing) error flow through internal states of LSTM units.\n",
    "- Local in space and time! O(1) update complexity per time step and weight.\n",
    "- An LSTM unit uses a set of gates to control what is entering and leaving the unit (input gate and output gate). Essentially, the unit learns when to keep, override or access information which we refer to as *cell state*.\n",
    "\n",
    "### LSTM Unit Structure\n",
    "\n",
    "#### Cell State (Latent State)\n",
    "\n",
    "The LSTM is structured so that can remove or add information to the *cell state*. *Gates* regulate the information flow within the LSTM.\n",
    "\n",
    "The flow of the *cell state* is shown in the following diagram:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png\", width=700>\n",
    "\n",
    "#### Forget Gate\n",
    "\n",
    "The *forget gate* applies a sigmoid function to the the previous output $h_{t-1}$ and input $x_t$. It outputs a number between 0 and 1 to decide which part of the previous *cell state* it wants to keep. Zero means \"don't let anything through\", one means \"let everything through\".\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png\", width=700>\n",
    "\n",
    "#### Input Gate\n",
    "\n",
    "The *input gate* controls the flow of new information into the new *cell state*. A *sigmoid layer* regulates which values we will update and a *tanh layer* proposes values that could be added to the state.\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png\", width=700>\n",
    "\n",
    "#### Cell State Update\n",
    "\n",
    "We update step of the *cell state* is simple. We multiply the old state $C_{t-1}$ by the output of the *forget layer* $f_t$ and add $i_t * \\tilde{C}_t$:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png\", width=700>\n",
    "\n",
    "#### Output Gate\n",
    "\n",
    "As a last step, we generate the LSTM unit's output:\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\", width=700>\n",
    "\n",
    "#### LSTM Modules\n",
    "\n",
    "<img src=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\", width=700>\n",
    "\n",
    "### LSTM Structure Variants\n",
    "\n",
    "* Peephole connections (Gers & Schmidhuber, 2000)\n",
    "* Gated Recurrent Units (Cho, et al., 2014)\n",
    "* Depth Gated RNNs (Yao, et al., 2015)\n",
    "* Clockwork RNNs (Koutnik, et al., 2014)\n",
    "* Highway Networks (Srivastava, Greff, Schmidhuber, 2015)\n",
    "\n",
    "See \"LSTM: A search space odyssey\" (Greff, et al., 2015) for a useful comparison of popular variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "Keras is a neural networks library that can run on either Theano or TensorFlow. Keras has been build for:\n",
    "\n",
    "- Modularity\n",
    "- Minimalism\n",
    "- Easy extensibility\n",
    "- Work with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Implementation of an LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from scipy import stats\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(25)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = datetime.datetime(1962, 1, 1)\n",
    "end = datetime.datetime(2016, 1, 1)\n",
    "\n",
    "DataReader(\"GE\", 'yahoo', start, end).to_csv(\"ge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timesteps = 5\n",
    "slide = 3\n",
    "prediction_steps = 1\n",
    "\n",
    "# Load Data\n",
    "df = read_csv('ge.csv')\n",
    "\n",
    "# series[0] is open price at 1.1.1962\n",
    "series = df['Open'].as_matrix()\n",
    "\n",
    "# Standardize data\n",
    "series = (series - np.mean(series)) / np.std(series)\n",
    "\n",
    "# Create starting indices window\n",
    "a = np.arange(0, len(series)-(timesteps+prediction_steps), slide)\n",
    "\n",
    "# Create window summation indices\n",
    "b = np.arange(0, timesteps + prediction_steps)\n",
    "\n",
    "# Create mask\n",
    "start_tiles = np.tile(a, (timesteps + prediction_steps, 1)).T\n",
    "sum_tiles = np.tile(b, (len(start_tiles), 1))\n",
    "index_mask = start_tiles + sum_tiles\n",
    "x_mask = index_mask[:,:-prediction_steps]\n",
    "y_mask = index_mask[:, -prediction_steps:]\n",
    "\n",
    "# Build data set\n",
    "x, y = series[x_mask], series[y_mask]\n",
    "\n",
    "# Split data\n",
    "split_idx = int(len(x) * 0.75)\n",
    "\n",
    "x_train = x[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "\n",
    "x_test = x[split_idx:]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "# Need to add one more dim to make batches\n",
    "x_train = np.expand_dims(x_train, 2)\n",
    "x_test = np.expand_dims(x_test, 2)\n",
    "\n",
    "print x_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(25, input_shape=(timesteps, 1), activation='sigmoid'))#, return_sequences=True))\n",
    "#model.add(LSTM(25, input_shape=(timesteps, 1), activation='sigmoid'))\n",
    "model.add(Dense(prediction_steps, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Learning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "\n",
    "print model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Training in Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=epochs, verbose=0)\n",
    "plt.figure(figsize=(35,10))\n",
    "plt.plot(np.arange(0, epochs), hist.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "y_pred = model.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "for i in range(prediction_steps):\n",
    "    rmse = np.sqrt(np.sum((y_test[:, i] - y_pred[:, i])**2))\n",
    "    print \"LSTM RMSE %i: %.4f\" % (i, rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Visualize Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def chow(X, Y, breakpoint, alpha = 0.05):\n",
    "    \"\"\"\n",
    "    Performs a chow test.\n",
    "    Split input matrix and output vector into two\n",
    "    using specified breakpoint.\n",
    "     X - independent variables matrix\n",
    "     Y - dependent variable vector\n",
    "     breakpoint - index to split.\n",
    "     alpha is significance level for hypothesis test\n",
    "    \"\"\"\n",
    "    k = len(X[0])\n",
    "    n = len(X)\n",
    "\n",
    "    # Split into two datasets.\n",
    "    X1 = X[:breakpoint][:]\n",
    "    Y1 = Y[:breakpoint][:]\n",
    "\n",
    "    #print X1.shape\n",
    "    #print Y1.shape\n",
    "\n",
    "    X2 = X[breakpoint:][:]\n",
    "    Y2 = Y[breakpoint:][:]\n",
    "    \n",
    "    #print X2.shape\n",
    "    #print Y2.shape\n",
    "\n",
    "    # Perform separate three least squares.\n",
    "    #allfit   = lm.ols(X,Y)\n",
    "    allfit = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
    "    #lowerfit = lm.ols(X1, Y1)\n",
    "    lowerfit = np.dot(np.dot(np.linalg.inv(np.dot(X1.T, X1)), X1.T), Y1)\n",
    "    #upperfit = lm.ols(X2, Y2)\n",
    "    upperfit = np.dot(np.dot(np.linalg.inv(np.dot(X2.T, X2)), X2.T), Y2)\n",
    "\n",
    "    RSS  = np.sum(np.square(np.dot(X, allfit) - Y)) #allfit.RSS\n",
    "    RSS1 = np.sum(np.square(np.dot(X1, lowerfit) - Y1)) #lowerfit.RSS\n",
    "    RSS2 = np.sum(np.square(np.dot(X2, upperfit) - Y2)) #upperfit.RSS\n",
    "\n",
    "    df1 = k\n",
    "    df2 = n - 2 *k\n",
    "\n",
    "    num = (RSS - (RSS1 + RSS2)) / float(df1)\n",
    "    den = (RSS1 + RSS2) / (df2)\n",
    "\n",
    "    Ftest = num/den\n",
    "    Fcrit = stats.f.ppf([1 -0.05], df1, df2)\n",
    "    return (Ftest, Fcrit, df1, df2, RSS, RSS1, RSS2)\n",
    "\n",
    "t = np.arange(0, len(y_pred[:1000, 0]))\n",
    "\n",
    "plt.figure(figsize=(35, 15))\n",
    "plt.plot(t, y_test.T.ravel()[:1000], 'b')\n",
    "plt.plot(t, y_pred.T.ravel()[:1000], 'r')\n",
    "plt.show()\n",
    "\n",
    "lower = 5\n",
    "upper = x_test.shape[0] - 2\n",
    "idx = lower\n",
    "Ftests = []\n",
    "Fcrits = []\n",
    "x_chow = x_test[:, :, 0]\n",
    "y_chow = y_test[:, 0]\n",
    "\n",
    "while idx <= upper:\n",
    "    (Ftest, Fcrit, df1, df2, RSS, RSS1, RSS2) = chow(x_chow, y_chow, idx)\n",
    "    Ftests.append(Ftest)\n",
    "    Fcrits.append(Fcrit[0])\n",
    "    idx += 1\n",
    "\n",
    "plt.figure(figsize=(35, 15))\n",
    "plt.plot(Ftests)\n",
    "plt.plot(Fcrits)\n",
    "plt.ylim([-5, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impulse Response of Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def random_color():\n",
    "    rgb = []\n",
    "    for i in range(3):\n",
    "        c = np.random.randint(0, 256, 1)\n",
    "        rgb += [c[0]]\n",
    "\n",
    "    return '#%02x%02x%02x' % tuple(rgb)\n",
    "\n",
    "'''\n",
    "Setting the activations functions\n",
    "'''\n",
    "# gate activation function (tanh)\n",
    "f = np.tanh\n",
    "# cell activation function (sigmoid)\n",
    "g = sigmoid\n",
    "# unit output activation\n",
    "h = np.tanh\n",
    "'''\n",
    "Initialize the plots\n",
    "'''\n",
    "width, height = 500, 300\n",
    "p_data = figure(title=\"Input\", plot_width=width, plot_height=height)\n",
    "p_unit = figure(title=\"Unit output\", plot_width=width, plot_height=height)\n",
    "p_out = figure(title=\"Output Gate\", plot_width=width, plot_height=height)\n",
    "p_cell = figure(title=\"Cell State\", plot_width=width, plot_height=height)\n",
    "p_forget = figure(title=\"Forget Gate\", plot_width=width, plot_height=height)\n",
    "p_input = figure(title=\"Input Gate\", plot_width=width, plot_height=height)\n",
    "'''\n",
    "Initialize loop\n",
    "'''\n",
    "repeat = 20\n",
    "T = 30\n",
    "time = np.arange(0, T)\n",
    "weight_dict = defaultdict(list)\n",
    "for _ in range(repeat):\n",
    "    '''\n",
    "    Initializing weights and states\n",
    "    '''\n",
    "    # unit input weights\n",
    "    w_i, w_f, w_c, w_o = np.random.standard_normal(4)\n",
    "    # recurrent connections\n",
    "    u_i, u_f, u_c, u_o = np.random.standard_normal(4)\n",
    "    # peephole connections\n",
    "    c_i, c_f, c_o = np.random.standard_normal(3)\n",
    "    # store input weights\n",
    "    weight_dict['w_i'] += [w_i]\n",
    "    weight_dict['w_f'] += [w_f]\n",
    "    weight_dict['w_c'] += [w_c]\n",
    "    weight_dict['w_o'] += [w_o]\n",
    "    # store recurrent conns\n",
    "    weight_dict['u_i'] += [u_i]\n",
    "    weight_dict['u_f'] += [u_f]\n",
    "    weight_dict['u_c'] += [u_c]\n",
    "    weight_dict['u_o'] += [u_o]\n",
    "    # store peephole connections\n",
    "    weight_dict['c_i'] += [c_i]\n",
    "    weight_dict['c_f'] += [c_f]\n",
    "    weight_dict['c_o'] += [c_o]    \n",
    "    '''\n",
    "    Simlate cell over multiple time steps with a spike at t=0\n",
    "    '''\n",
    "    a_i, a_f, a_o = np.zeros(T), np.zeros(T), np.zeros(T)\n",
    "    s_c, a_b = np.zeros(T+1), np.zeros(T+1)\n",
    "    x = np.zeros(T)\n",
    "    x[0] = 1\n",
    "    for m, t in zip(range(T), range(1, T+1)):\n",
    "        # input gate\n",
    "        z_i = w_i * x[m] + u_i * a_b[t-1] + c_i * s_c[t-1]\n",
    "        a_i[m] = f(z_i)\n",
    "        # forget gate\n",
    "        z_f = w_f * x[m] + u_f * a_b[t-1] + c_f * s_c[t-1]\n",
    "        a_f[m] = f(z_f)\n",
    "        # cell state\n",
    "        z_c = w_c * x[m] + u_c * a_b[t-1]\n",
    "        s_c[t] = a_f[m] * s_c[t-1] + a_i[m] * g(z_c)\n",
    "        # output gate\n",
    "        z_o = w_o * x[m] + u_o * a_b[t-1] + c_o * s_c[t]\n",
    "        a_o[m] = f(z_o)\n",
    "        # unit output\n",
    "        a_b[t] = a_o[m] * h(s_c[t])\n",
    "\n",
    "    # delete the initial values\n",
    "    s_c = s_c[1:]\n",
    "    a_b = a_b[1:]\n",
    "    # store final values\n",
    "    weight_dict['a_i'] += [a_i[-1]]\n",
    "    weight_dict['a_f'] += [a_f[-1]]\n",
    "    weight_dict['a_o'] += [a_o[-1]]\n",
    "    weight_dict['a_b'] += [a_b[-1]]\n",
    "    weight_dict['s_c'] += [s_c[-1]]\n",
    "    # plot\n",
    "    p_data.line(time, x, color=random_color())\n",
    "    p_unit.line(time, a_b, color=random_color())\n",
    "    p_out.line(time, a_o, color=random_color())\n",
    "    p_cell.line(time, s_c, color=random_color())\n",
    "    p_forget.line(time, a_f, color=random_color())\n",
    "    p_input.line(time, a_i, color=random_color())\n",
    "    \n",
    "# show the plotq\n",
    "grid = [[p_data, p_input], [p_forget, p_cell], [p_out, p_unit]]\n",
    "show(gridplot(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "* RNN and LSTM images taken from http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* Supervised Sequence Labelling with Recurrent Neural Networks (Graves): https://www.cs.toronto.edu/~graves/preprint.pdf\n",
    "* Neural Networks and Deep Learning: http://www.neuralnetworksanddeeplearning.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
